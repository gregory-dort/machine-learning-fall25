{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97cb0e8c",
   "metadata": {},
   "source": [
    "Importing necessary libraries / scikitlearn models here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf10c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, SimpleRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn._loss import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7cf118",
   "metadata": {},
   "source": [
    "Loading dataset from url link and creating dataframe. Normalizing data here as well by using Sunspots to make prediction and setting a range of 0-1 for the data being represented. Splitting normalized data into 80-20 test-train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d870661",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "data = df['Sunspots'].values.reshape(-1, 1)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "test_data = scaled_data[train_size:]\n",
    "train_data = scaled_data[:train_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf2b742",
   "metadata": {},
   "source": [
    "Sequence Creation Function (Sliding Window Concept) to analyze current sequence and predict next item for the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(dataset, look_back=1):\n",
    "    x, y = [], []\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        x.append(a)\n",
    "        y.append(dataset[i + look_back, 0])\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90601b3",
   "metadata": {},
   "source": [
    "Defining the look back period and reshaping input format to be [samples, time steps, features] for the LSTM RNN Model. Reshaping input for dense to [samples, features] (time steps is flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e83bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look Back set to 11 to remain within 60-70 parameter limit, if it were 12 the fully connected model would have 71 parameters\n",
    "look_back = 11\n",
    "\n",
    "x_train, y_train = create_sequences(train_data, look_back)\n",
    "x_test, y_test = create_sequences(test_data, look_back)\n",
    "\n",
    "# Reshape input for LSTM RNN\n",
    "x_train_lstm = np.reshape(x_train, (x_train.shape[0],x_train.shape[1], 1))\n",
    "x_test_lstm = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Reshape for Dense\n",
    "x_train_dense = x_train\n",
    "x_test_dense = x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb79201",
   "metadata": {},
   "source": [
    "Building Both RNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f96b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model A: Fully Connected RNN Model ---\n",
    "# 11 Inputs -> 5 Neurons -> 1 Output\n",
    "# Params: (11*5 + 5) + (5*1 + 1) = 60 + 6 = 66 Parameters\n",
    "model_fc = Sequential([\n",
    "    Dense(5, input_dim=look_back, activation=tf.nn.leaky_relu),\n",
    "    Dense(1)\n",
    "])\n",
    "model_fc.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# --- Mdel B: LSTM RNN Model ---\n",
    "# 3 Units\n",
    "# Params: 4 * ((1 feature + 1 bias) * 3 units + 3 units^2) = 60 LSTM params\n",
    "# Output Dense: 3 inputs * 1 output + 1 bias = 4 dense params\n",
    "# Total: 64 Parameters\n",
    "model_lstm = Sequential([\n",
    "    LSTM(3, input_shape=(look_back, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "model_lstm.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7013d",
   "metadata": {},
   "source": [
    "Verifying Parameters, training Both Models, Creating Predictions / Evals and Inverse Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying Params Counts\n",
    "print(f\"Fully Connected Params: {model_fc.count_params()}\")\n",
    "print(f\"LSTM Params: {model_lstm.count_params()}\")\n",
    "\n",
    "# Training Models\n",
    "print(\"\\nTraining Fully Connected Model...\")\n",
    "model_fc.fit(x_train_dense, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "print(\"\\nTraining LSTM Model...\")\n",
    "model_lstm.fit(x_train_lstm, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Creating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e50951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnnvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
